{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c8d38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's break down the code for Step 2 (building the neural network) in more detail:\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Split the Training and Test Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Create a Sequential Model\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# 3. Add Layers to Your Model\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu', input_shape=(9,)))  # Input Layer\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))                  # Hidden Layer 1\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))                   # Hidden Layer 2\n",
    "model.add(tf.keras.layers.Dense(3, activation='softmax'))                  # Output Layer\n",
    "\n",
    "\n",
    "# 4. Compile the Model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 5. Fit the Training Data\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# 6. Evaluate the Model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# 7. Save the Model\n",
    "model.save(\"tic-tac-toe.model\")\n",
    "\n",
    "'''\n",
    "This code snippet is a simple implementation of a neural network using TensorFlow and Keras to play Tic-Tac-Toe. The model is built using a sequential architecture, which is a linear stack of layers. Here's a detailed explanation of each part of the code:''\n",
    "\n",
    "Detailed Explanation:\n",
    "\n",
    "1. Splitting Data:\n",
    "\n",
    "train_test_split(X, y, test_size=0.2, random_state=42): This line splits your data into training and testing sets.\n",
    "\n",
    "X: Your input features (the Tic-Tac-Toe board states). It should be a NumPy array or a similar data structure.\n",
    "\n",
    "y: Your target variable (the outcome of the game or the move to make). Also a NumPy array or similar.\n",
    "\n",
    "test_size=0.2: 20% of your data is used for testing, 80% for training.\n",
    "\n",
    "random_state=42: This ensures the split is the same every time you run the code, making results reproducible. You can use any integer.\n",
    "\n",
    "2. Creating a Sequential Model:\n",
    "\n",
    "model = tf.keras.Sequential(): This creates a linear stack of layers for your neural network. Data flows sequentially through each layer.\n",
    "\n",
    "3. Adding Layers:\n",
    "\n",
    "3.a Input Layer:\n",
    "\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu', input_shape=(9,))\n",
    "\n",
    "tf.keras.layers.Dense: This creates a fully connected (dense) layer where each neuron is connected to every neuron in the previous and next layers. It's the most common type of layer.\n",
    "\n",
    "64: This layer has 64 neurons (units). This is a hyperparameter you can tune.\n",
    "\n",
    "activation='relu': The Rectified Linear Unit (ReLU) activation function introduces non-linearity into the model. ReLU outputs the input if positive, otherwise, it outputs zero. relu is a good default choice for hidden layers.\n",
    "\n",
    "input_shape=(9,): This specifies the shape of the input data. (9,) means each input sample is a vector of 9 features. This is crucial for the first layer to know the expected input dimensions. In the tic-tac-toe example, we have a 3x3 board, which means 9 possible positions, hence we use 9 as the input_shape.\n",
    "\n",
    "3.b Hidden Layers:\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "\n",
    "These lines add two more hidden layers with 128 and 64 neurons, respectively, also using ReLU activation. You can add or remove hidden layers and change the number of neurons as needed.\n",
    "\n",
    "3.c Output Layer:\n",
    "\n",
    "model.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "3: This layer has 3 neurons, corresponding to the 3 possible outcomes in Tic-Tac-Toe (win, loss, draw) for a given player.\n",
    "\n",
    "activation='softmax': The softmax function converts the raw output of the last layer into probabilities that sum up to 1. This is essential for multi-class classification problems. Each probability represents the likelihood of a particular outcome.\n",
    "\n",
    "4. Compiling the Model:\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "optimizer='adam': The Adam optimizer is a popular algorithm for training neural networks. It efficiently adjusts the model's weights to minimize the loss function.\n",
    "\n",
    "loss='sparse_categorical_crossentropy': This loss function is used for multi-class classification problems when your labels are integers (0, 1, 2,...). It measures the difference between the predicted probabilities and the true labels.\n",
    "\n",
    "If you used one-hot encoding for your target variable, you'd use categorical_crossentropy instead.\n",
    "\n",
    "metrics=['accuracy']: This tells the model to track accuracy during training and evaluation.\n",
    "\n",
    "5. Training the Model:\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32):\n",
    "\n",
    "X_train, y_train: The training data and labels.\n",
    "\n",
    "epochs=10: The model will iterate over the entire training dataset 10 times.\n",
    "\n",
    "batch_size=32: The model processes 32 samples at a time before updating its weights. Smaller batch sizes can lead to more stable training but take longer. Larger batch sizes are faster but might lead to less stable training.\n",
    "\n",
    "6. Evaluating the Model:\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0): This calculates the loss and accuracy of the model on the test data.\n",
    "\n",
    "verbose=0 suppresses the progress bar during evaluation. Use verbose=1 or verbose=2 to see the progress.\n",
    "\n",
    "The print statements display the results.\n",
    "\n",
    "7. Saving the Model:\n",
    "\n",
    "model.save(\"tic-tac-toe.model\"): Saves the trained model to a file so you can load and reuse it later without retraining. The newer method is model.save(\"tic_tac_toe_model\", format='tf') which produces a directory instead of a single file to represent the model. To load it, you will use tf.keras.models.load_model(\"tic_tac_toe_model\").\n",
    "\n",
    "This detailed explanation should provide a clearer understanding of each line of code and its purpose in building your Tic-Tac-Toe neural network. Remember that you will need to adapt the input and output shapes and potentially other hyperparameters (number of layers, neurons, etc.) based on your specific implementation of the Tic-Tac-Toe game.\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
